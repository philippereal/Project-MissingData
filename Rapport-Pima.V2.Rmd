---
title: "Rapport - Données manquantes"
output:
  html_document:
    df_print: paged
---

#### Nom et Prénom des étudiants du groupe :

```
- Nom : Prénom : 
- Nom : Prénom : 
```

### 1. Introduction

Présentation des données et objectifs de l'étude

https://www.kaggle.com/uciml/pima-indians-diabetes-database

* Context
This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.
  
* Content
The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

* Objectif
Prédire si l'individu a le diabête ou pas.
Construire un modèle dans ce but.
Auparavant compléter les données manquante par imputation multiple.

Columns

npreg            : PregnanciesNumber of times pregnant
glu              : GlucosePlasma glucose concentration a 2 hours in an oral glucose tolerance test
bp               : BloodPressureDiastolic blood pressure (mm Hg)
skin             : SkinThicknessTriceps skin fold thickness (mm)
Insuline         : Insulin2-Hour serum insulin (mu U/ml)
bmi              : BMIBody mass index (weight in kg/(height in m)^2)
ped              : DiabetesPedigreeFunctionDiabetes pedigree function
age              : AgeAge (years)
type             : OutcomeClass variable (0 or 1) 268 of 768 are 1, the others are 0

* Librayries
```{r echo=FALSE} 
library(missMDA);
library(mice);
library(VIM)
library(FactoMineR)
library(ggplot2)
library(factoextra)
library(corrplot)
```

#### Chargement des donnés - Résumé

```{r} 
data<-read.table("Pima_DataWithNA.txt",sep=";" ,header=T, na.strings = "NA")
#View(data)
summary(data)
```

* Traitement des données

```{r} 
#suppression de la 1er colonne (Id) et de la 6ème (Insuline)=> trop de données manquantes (peourrait être considérés dans un 2nd tps)
tab_Pima1<-data[,-1]
tab_Pima1<-tab_Pima1[,-5]
maxDim<-8 #8 si on décide de supprimer la colonne insuline 9 sinon

data_Pima1<-as.data.frame.matrix(tab_Pima1)

data_Pima1$type <- as.logical(data_Pima1$type)

dataPima<-data_Pima1[,1:maxDim] 

#View(data_Pima1)
```

**********

### 2. Exploration des données

	https://www.kaggle.com/laozhang/statistical-learning-with-r				
	https://www.kaggle.com/naveenkhasa/pima-indian-diabetes-dataset			
	`



#### Classification des données manqunates: MCAR/MNAR

https://datascienceplus.com/imputing-missing-data-with-r-mice-package/

Quick classification of missing data
There are two types of missing data:

MCAR: missing completely at random. This is the desirable scenario in case of missing data.
MNAR: missing not at random. Missing not at random data is a more serious issue and in this case it might be wise to check the data gathering process further and try to understand why the information is missing. For instance, if most of the people in a survey did not answer a certain question, why did they do that? Was the question unclear?
Assuming data is MCAR, too much missing data can be a problem too. Usually a safe maximum threshold is 5% of the total for large datasets. If missing data for a certain feature or sample is more than 5% then you probably should leave that feature or sample out. We therefore check for features (columns) and samples (rows) where more than 5% of the data is missing using a simple function



```{r, include=FALSE} 
data<-dataPima
pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(data,2,pMiss)
apply(data,1,pMiss)
```


Using mice for looking at missing data pattern
The mice package provides a nice function md.pattern() to get a better understanding of the pattern of missing data

```{r fig.width=10}
library(mice)
md.pattern(data)
```

A perhaps more helpful visual representation can be obtained using the VIM package as follows

```{r} 
library(VIM)
aggr_plot <- aggr(dataPima, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(dataPima), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

Another (hopefully) helpful visual approach is a special box plot

```{r} 
marginplot(data[c(4,5)])
```

### 3. Imputation

Dans un premier temps on traite les valeurs manquantes.
A partir des jeux de données complétés par imputation (test de différents méthodes) simple ou multiple on construit un modèle de type logit pour la variable réponse.
Dans notre cas Le patient est diabétique ou pas.

#### 3.1 Imputation simple


#### 3.2 Imputation multiple avec MIPCA

* Détermination de la dimension de l'espace de projection
```{r nb.kfold, echo=FALSE, include=FALSE} 
## First the number of components has to be chosen 
nb_Pima1 <- estim_ncpPCA(data_Pima1[,1:maxDim])
nb.kfold <- estim_ncpPCA(data_Pima1[,1:maxDim], method.cv = "Kfold")
```

```{r nb.kfold_PLOT} 
nb.kfold$ncp
nb_Pima1$ncp
plot(names(nb.kfold$criterion),nb.kfold$criterion,type="b")

```


nb_Pima1 = 1
nb.kfold$ncp = 5 avec insuline=>4

big différence!
Les résultats sont meilleur avec 5

Peut être utilser 4 

```{r settings} 
#setting des variables 
ncp.res<-nb.kfold$ncp 
#ncp.res<-nb_Pima1$ncp
dataImp<-data_Pima1[,1:maxDim] 
dataPima<-data_Pima1[,1:maxDim]
```

#### 3.2.1 Multiple Imputation with Bayesian method
```{r eval=FALSE, include=FALSE} 
## Multiple Imputation with Bayesian method

res.BayesMIPCA_Pima1<-MIPCA(dataPima,ncp=ncp.res,verbose=TRUE,method.mi = "Bayes")
```

```{r eval=FALSE, include=FALSE, fig.height=10, fig.width=10}
## Bayesian method
plot(res.BayesMIPCA_Pima1)
```
En changeant le nb de dimensions : de 1 à 5 le résultat est meilleur
Pb pour la variabble ped mais peu d'individus concernés => ped>1.5

```{r eval=FALSE, include=FALSE,  fig.height=10, fig.width=10}
## Diagnostics
res.over_Pima1<-Overimpute(res.BayesMIPCA_Pima1)
```

#### 3.2.2 Multiple Imputation with Bayesian method et EM

```{r eval=FALSE, include=FALSE}
res.EMBayesMIPCA_Pima1<-MIPCA(dataPima,ncp=ncp.res,verbose=TRUE,method= "EM", method.mi = "Bayes")

```

=> pas vraiment d'améliorations

```{r eval=FALSE, include=FALSE,  fig.height=10, fig.width=10}
res.over_EMBayes_Pima1<-Overimpute(res.EMBayesMIPCA_Pima1)
```


#### 3.2.3 Multiple Imputation with Boost method

Le résultat semble meilleur

```{r eval=FALSE, include=FALSE}
# Boost method a la place de Bayesien
res.BootMIPCA_Pima1<-MIPCA(dataPima,ncp=ncp.res,verbose=TRUE,method.mi = "Boot")
```


```{r eval=FALSE, include=FALSE,  fig.height=10, fig.width=10}
## Diagnostics
res.overBoot_Pima1<-Overimpute(res.EMBootMIPCA_Pima1)
```


#### 3.2.4 Multiple Imputation with MIPCA Boost method

```{r MIPCA, include=FALSE } 
# Boost method a la place de Bayesien
res.EMBootMIPCA_Pima1<-MIPCA(dataPima,ncp=ncp.res,verbose=TRUE,method= "EM",method.mi = "Boot")
```

Le obtenu par boostrap et method= "EM" semble le meilleur résultat!!!! Yeaaah l'espoir rené!

```{r Diagnostics, fig.height=10, fig.width=10}
## Diagnostics
res.overEMBoot_Pima1<-Overimpute(res.EMBootMIPCA_Pima1)
```

```{r plotRes, fig.height=10, fig.width=10}
## Diagnostics
plot(res.EMBootMIPCA_Pima1)
```

```{r plotRes, fig.height=10, fig.width=10}
##tager les donn"es qui ont ete complétées
which(dataPima[,4]="NA")
```

* Inspecting the distribution of original and imputed data

Let’s compare the distributions of original and imputed data using a some useful plots.
First of all we can use a scatterplot and plot Ozone against all the other variables

```{r tempData} 
dataPima_NA<-data_Pima1[,1:maxDim]
#mi<-res.BayesMIPCA_Pima1
mi.res<-res.EMBootMIPCA_Pima1

tempData_MIPCA<-prelim(res.mi=mi.res,X=dataPima_NA)#creating a mids object
#help("prelim")
#data_MIPCA<-res.EMBootMIPCA_Pima1$res.imputePCA
#DataMat=data.matrix(data_MIPCA)
#frameData_MIPCA<-as.data.frame(data_MIPCA)
#frameData_MIPCA$type <- as.logical(frameData_MIPCA$type)

#tempData_MIPCA<-prelim(res.mi=mi.res,X=frameData_MIPCA)#creating a mids object
```

```{r densityplot} 
xyplot(tempData_MIPCA, type ~ npreg + glu + bp + skin + bmi +  ped + age, pch=18,cex=1)
densityplot(tempData_MIPCA)
stripplot(tempData_MIPCA, pch = 20, cex = 1.2)

```


```{r stripplot} 
```


#### 3.3 Imputation multiple autres méthodes

Finalement le MIFAMD ne semble pas approprié
Peut-être tester d'autres méthodes du package MICE par exemple...

https://datascienceplus.com/imputing-missing-data-with-r-mice-package/

```{r, echo=FALSE} 
tempData <- mice(dataPima,m=5,maxit=50,meth='pmm',seed=500)
```

```{r, echo=FALSE} 
summary(tempData)
```

```{r, echo=FALSE} 
completedData_mice <- complete(tempData,1)
xyplot(tempData,type ~ npreg + glu + bp + skin + bmi +  ped + age,pch=18,cex=1)
densityplot(tempData)
stripplot(tempData_MIPCA, pch = 20, cex = 1.2)

```

#### 4.Comparaison données complétées aux données de la librairie MASS (sous échantillon)

```{r, echo=FALSE} 
View(dataFramePima)
dataPCA<-res.EMBootMIPCA_Pima1$res.imputePCA
#DataMat=data.matrix(dataPCA)
dataFramePima<-as.data.frame(dataPCA)
dataFramePima$type <- as.logical(dataFramePima$type)

summary(dataFramePima)
summary(completedData_mice)
summary(Pima.tr)
```

##### Etude des Correlations

Correlations très voisines, tendance à baisser pour les données imputés (sauf pour corr bmi / skin )
La structure globale des corrélations est très voisine.

```{r echo=FALSE, message=FALSE, warning=FALSE}
require(corrplot)

dataPCA<-res.EMBootMIPCA_Pima1$res.imputePCA
DataMat=data.matrix(dataPCA)
mcor<-cor(DataMat)
corrplot(mcor, type="upper",tl.col="black", tl.srt=8)

library("MASS")
dataPima.tr<-data(Pima.tr)
#data(Pima.te)
DataMat.mass=data.matrix(Pima.tr)
mcor.mass<-cor(DataMat.mass)
corrplot(mcor.mass, type="upper",  tl.col="black", tl.srt=8)


DataMat_mice=data.matrix(completedData_mice)
mcor_mice<-cor(completedData_mice)
corrplot(mcor_mice, type="upper",tl.col="black", tl.srt=8)
```


```{r PCA, echo=FALSE, fig.height=10, fig.width=10}
## Diagnostics
dataPCA<-res.EMBootMIPCA_Pima1$res.imputePCA
res.pca<-PCA(dataPCA,graph=FALSE, quali.sup = maxDim) 
summary(res.pca) 
```


```{r PCA_MASS, echo=FALSE, fig.height=10, fig.width=10}
## Diagnostics
library("MASS")
dataPima.tr<-data(Pima.tr)
#data(Pima.te)

res.pca.mass<-PCA(Pima.tr,graph=FALSE, quali.sup = 8) 
summary(res.pca.mass) 
```

```{r PCA011, echo=FALSE, fig.width=10}
barplot(res.pca$eig[,2],main="Eigenvalues",names.arg=1:nrow(res.pca$eig))

barplot(res.pca.mass$eig[,2],main="Eigenvalues",names.arg=1:nrow(res.pca.mass$eig))
```


```{r meteoTrain_PCA0+, fig.height=7, fig.width=10,  echo=FALSE}
#Graphique du nuage des individus - qualité de projection sur le plan principal > 0.7
#color <- ifelse(dataPCA[,9] == TRUE,"#E7B800","#00AFBB")

plot(res.pca, choix="var", axes=1:2,legend="Pima")
plot(res.pca,habillage=maxDim,cex=0.6, select="cos2 0.7", axes = 1:2)
```
```{r meteoTrain_PCA0+, fig.height=7, fig.width=10,  echo=FALSE}
#Graphique du nuage des individus - qualité de projection sur le plan principal > 0.7
#color <- ifelse(dataPCA[,9] == TRUE,"#E7B800","#00AFBB")

plot(res.pca.mass, choix="var", axes=1:2,legend="Pima")
plot(res.pca.mass,habillage=8,cex=0.6, select="cos2 0.7", axes = 1:2)
```

### 4.2 Analyses automatiques

#### 4.2.1 coefficients de corrélation entre chacunes des variables et les 5 premières composantes principales 
(ce qui correspond aux coordonnées des individus sur les 5 premiers axes)

##### A partir de la fonction dimdesc

```{r 4.2.1, fig.height=15, fig.width=10, include=TRUE}
dimdesc <- dimdesc(res.pca, proba=1e-5) #, nbelements = 16)
dimdesc
```

##### Graphique de corrélation avec corrplot

```{r PCAcor_var_dim1, echo=FALSE, fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
cor_var_dim<-round(res.pca$var$coord[,1:5],2) 
cor_var_dim.mass<-round(res.pca.mass$var$coord[,1:5],2) 
cor_var_dim
cor_var_dim.mass

corrplot(cor_var_dim, is.corr = FALSE, tl.col="black", tl.srt=maxDim)
corrplot(cor_var_dim.mass, is.corr = FALSE, tl.col="black", tl.srt=8)
```


#### 4.2.2 Indice de qualité de la représentation cos2

```{r cos2_var_dim1, echo=FALSE, message=FALSE, warning=FALSE}
cos2_var_dim<-round(res.pca$var$cos2[,1:5],2) 
cos2_var_dim.mass<-round(res.pca.mass$var$cos2[,1:5],2) 
cos2_var_dim
cos2_var_dim.mass
corrplot(cos2_var_dim, is.corr = FALSE, method="circle", tl.col="black", tl.srt=maxDim)
corrplot(cos2_var_dim.mass, is.corr = FALSE, method="circle", tl.col="black", tl.srt=8)
```


#### 4.2.3 contribution des variables à la construction des axes

```{r PCAcontrib_var_dim2, echo=FALSE}
contrib_var_dim<-round(res.pca$var$contrib[,1:5],2) 
contrib_var_dim.mass<-round(res.pca.mass$var$contrib[,1:5],2) 

contrib_var_dim
contrib_var_dim.mass
corrplot(contrib_var_dim, is.corr = FALSE, tl.col="black", tl.srt=maxDim)
corrplot(contrib_var_dim.mass, is.corr = FALSE, tl.col="black", tl.srt=8)
```

**********

### 4. Modélisation de la variable réponse

Regression on the multiply imputed data set and pooling with mice


```{r Modele} 
dataPima<-data_Pima1[,1:maxDim] #dataImp
#mi<-res.BayesMIPCA_Pima1
mi.res<-res.EMBootMIPCA_Pima1

#modele1<-glm(type ~ npreg + glu + bp + skin + bmi +  ped + age, family = binomial)
#modele2<-glm(type ~ npreg + glu + ped, family = binomial)

imp_Pima1<-prelim(res.mi=mi.res,X=dataPima)#creating a mids object

#MICE: with.mids Evaluate an expression in multiple imputed datasets
fit_Pima1_m1 <- with(data=imp_Pima1,exp=glm(type ~ npreg + glu + bp + skin + bmi +  ped + age,family=binomial))
fit_Pima1_m2 <- with(data=imp_Pima1,exp=glm(type ~ npreg + glu + bp + bmi +  ped + age ,family=binomial))
fit_Pima1_m3 <- with(data=imp_Pima1,exp=glm(type ~ npreg + glu + skin + bmi +  ped + age,family=binomial))
fit_Pima1_m4 <- with(data=imp_Pima1,exp=glm(type ~ npreg + glu + bmi +  ped + age,family=binomial))
fit_Pima1_m5 <- with(data=imp_Pima1,exp=glm(type ~ npreg + glu + bmi +  ped ,family=binomial))
fit_Pima1_m6 <- with(data=imp_Pima1,exp=glm(type ~ npreg + glu + bmi +  ped ,family=binomial))
fit_Pima1_m7 <- with(data=imp_Pima1,exp=glm(type ~ npreg + glu + ped ,family=binomial))
fit_Pima1_m8 <- with(data=imp_Pima1,exp=glm(type ~ glu + ped ,family=binomial))

#anova.imp <- with(data=imp_Pima1,exp=anova(lm(type ~ npreg + glu + bp + skin + bmi +  ped + age)))



#res.pool_Pima1_anova<-pool(anova.imp);

```

#### Choix du modèle

```{r} 
anova(fit_Pima1_m4,fit_Pima1_m5)
anova(fit_Pima1_m4,fit_Pima1_m3)
anova(fit_Pima1_m4,fit_Pima1_m3)

```


```{r anova} 
anova(fit_Pima1_m1,fit_Pima1_m2)
anova(fit_Pima1_m1,fit_Pima1_m3)
anova(fit_Pima1_m1,fit_Pima1_m4)
anova(fit_Pima1_m1,fit_Pima1_m5)
anova(fit_Pima1_m1,fit_Pima1_m6)
anova(fit_Pima1_m1,fit_Pima1_m7)
anova(fit_Pima1_m1,fit_Pima1_m8)
anova(fit_Pima1_m6,fit_Pima1_m8)
anova(fit_Pima1_m7,fit_Pima1_m8)

```

Choix du plus grand modèle?

#### Choix du modèle - Utilisation de regsubsets à partir du modèle réduit m_PCA

```{r message=FALSE, warning=FALSE, include=FALSE}
library(leaps)
dataPCA<-res.EMBootMIPCA_Pima1$res.imputePCA
#DataMat=data.matrix(dataPCA)
dataFramePima<-as.data.frame(dataPCA)
dataFramePima$type <- as.logical(dataFramePima$type)

choix_modele<-regsubsets(type ~ npreg + glu + bp + skin + bmi +  ped + age,
int=T, nbest=1,nvmax=4,method="exhaustive",data=dataFramePima)
#summary(choix_modele)
```
Là encore le résultat n'est pas satisfaisant.
```{r echo=FALSE, fig.height=8, fig.width=9}
par(mfrow=c(2,2))
plot(choix_modele,scale="r2", main="Choix de modèle - critère R2", cex.axis=0.7)
plot(choix_modele,scale="adjr2", main="Choix de modèle - critère R2 ajusté", cex.axis=0.7)
plot(choix_modele,scale="Cp", main="Choix de modèle - critère Cp de Mallows", cex.axis=0.7)
plot(choix_modele,scale="bic", main="Choix de modèle - critère BIC", cex.axis=0.7)
```

La méthode step du package leaps nous fait choisir un modèle à 4 variables explicatives: npreg + glu + skin + ped

#### Choix du modèle - méthode step du package MASS à partir du modèle saturé
On reprend le modèle saturé obtenu au §2: m_sature
```{r  include= TRUE}

m_sature = glm(formula = type ~ (npreg + glu + bp + skin + bmi +  ped + age),  family = binomial(link="logit"),  data = dataFramePima)
summary(m_sature)

```
#### Choix du modèle - Méthode progressive - step backward-forward à partir de m_sature

* Critère AIC
```{r eval=FALSE}
library(MASS)
modele_step_BwdFwd_AIC <- step(m_sature, data=dataFramePima, direction="both")
summary(modele_step_BwdFwd_AIC)
help("step")
```

* Critère BIC
```{r eval=FALSE}
K<-log(dim(dataFramePima)[1])
m_StepBwdFwd_BIC <- step(m_sature, data=dataFramePima, direction="both",k=K)
summary(m_StepBwdFwd_BIC)
#help("step")
```


##### Choix du modèle - Modèle obtenu
```{r include=TRUE}
modele_step_BwdFwd_AIC<-glm(formula = type ~ (npreg + glu + bp + skin + bmi + ped + age), family = binomial, data = dataFramePima)
AIC(modele_step_BwdFwd_AIC)
BIC(modele_step_BwdFwd_AIC)

m_StepBwdFwd_BIC<-glm(formula = type ~ (npreg + glu + skin + ped),family = binomial, data = dataFramePima)
AIC(m_StepBwdFwd_BIC)
BIC(m_StepBwdFwd_BIC)

```

Avec le critère AIC on obtient le modèle saturé
Avec le critère BIC on obtient à nouveau le modèle à 4 variable: npreg + glu + skin + ped


#### Pooling

https://datascienceplus.com/imputing-missing-data-with-r-mice-package/

```{r Pooling} 
res.pool_Pima1_m1<-pool(fit_Pima1_m1);
res.pool_Pima1_m2<-pool(fit_Pima1_m2);
res.pool_Pima1_m3<-pool(fit_Pima1_m3);
res.pool_Pima1_m4<-pool(fit_Pima1_m4);
res.pool_Pima1_m5<-pool(fit_Pima1_m5);
res.pool_Pima1_m6<-pool(fit_Pima1_m6);
res.pool_Pima1_m7<-pool(fit_Pima1_m7);
res.pool_Pima1_m8<-pool(fit_Pima1_m8);
```

```{r summaryPool} 
summary(res.pool_Pima1_m1)#pooling
summary(res.pool_Pima1_m2)#pooling
summary(res.pool_Pima1_m3)#pooling
summary(res.pool_Pima1_m4)#pooling
summary(res.pool_Pima1_m5)#pooling
summary(res.pool_Pima1_m6)#pooling
summary(res.pool_Pima1_m7)#pooling
summary(res.pool_Pima1_m8)#pooling
```





**********


### 5. Conclusion


```{r} 
```

Intérêt :
Les données Pima sont très utilsées (cf. Kaggle) pour autant il ne semble pas qu'un modèle d'inputation ait été utilisé.
Bien souvent les données manquantes sont supprimés ou bien imputatikon simple par moyenne.



